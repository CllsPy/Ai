## Notes

**GD**
Gradient Descent é um algorítimo para encontrar o valor máixmo ou mínimo de uma função, em ML ou DP usamos para minimizar a função de custo.

Uma função de custo é um algorítimo para avaliar o quão bem nosso modelo performou no dataset ou seja quão bem ele aprendeu o padrão.

**Mini Batch SGD**

[Mini Batch SGD](https://www.youtube.com/watch?v=FpDsDn-fBKA) é usado quando temos uma grande quantidade de dados.

Mini Batch porque ao invés da atualização dos pesos (w) ocorrer após a análise sobre todas as amostras ela ocorre a cada número de Batch (pedaços).

**How Gradiente Descent Works?**

## Useful resources
- [Tutorial 12- Stochastic Gradient Descent vs Gradient Descent](https://www.youtube.com/watch?v=FpDsDn-fBKA)
- [Gradient descent Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)

## Blog posts mentioned
- [Gradient Descent Algorithm — a deep dive](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)
- [Introduction to Loss Functions](https://www.datarobot.com/blog/introduction-to-loss-functions/)
